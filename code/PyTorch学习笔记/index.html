<!-- build time:Sun Oct 26 2025 10:52:07 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" href="http://amentiraz.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" href="http://amentiraz.github.io/atom.xml"><link rel="alternate" type="application/json" href="http://amentiraz.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="学习笔记,深度学习,PyTorch"><link rel="canonical" href="http://amentiraz.github.io/code/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><title>PyTorch学习笔记 - PyTorch - 代码 | Amentiraz =</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">PyTorch学习笔记</h1><div class="meta"><span class="item" title="创建时间：2024-11-25 14:55:21"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-11-25T14:55:21+08:00">2024-11-25</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>2.7k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>2 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Amentiraz</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124538.jpg"></li><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124532.jpg"></li><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124526.jpg"></li><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124500.png"></li><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124522.jpg"></li><li class="item" data-background-image="https://amentirazblogpic.oss-cn-hangzhou.aliyuncs.com/blogpic/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925162732.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/code/" itemprop="item" rel="index" title="分类于 代码"><span itemprop="name">代码</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/code/PyTorch/" itemprop="item" rel="index" title="分类于 PyTorch"><span itemprop="name">PyTorch</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://amentiraz.github.io/code/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Lemon Sour"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content=""></span><div class="body md" itemprop="articleBody"><p>基于 Pytorch 官方教程</p><span id="more"></span><p>Tensors (张量)，可以使用 GPU 进行计算<br>改变张量大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>,<span class="number">8</span>) <span class="comment">#2,8</span></span><br></pre></td></tr></table></figure><p>使用.item () 获取 value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure><h1 id="pytorch自动微分"><a class="anchor" href="#pytorch自动微分">#</a> PyTorch 自动微分</h1><p>如果想计算导数可以调用 Tensor.backward (). 如果 Tensor 是标量，即它包含一个元素数据，则不需要指定任何参数 backward, 但是如果它有更多元素，则需要指定一个 gradient 参数，来指定张量的形状.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = x + <span class="number">2</span> </span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line">out=z.mean()</span><br><span class="line"><span class="built_in">print</span>(z,out)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = y.mean()</span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><h1 id="pytorch-神经网络"><a class="anchor" href="#pytorch-神经网络">#</a> PyTorch 神经网络</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="variable language_">self</span>.num_flat_features(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:] <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">       num_features = <span class="number">1</span></span><br><span class="line">       <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>我们自己定义了一个前馈函数，然后反向传播函数被自动通过 autograd 定义了。可以使用任何张量操作在前馈函数上。</p><p>一个模型可训练的参数可以通过调用 net.parameters () 返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br></pre></td></tr></table></figure><p>把所有参数梯度缓存器置零，用随机的梯度来反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>损失函数：<br>一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。<br>有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss, 这计算了均方误差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)</span><br><span class="line">target = target.view(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output,target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><p>现在，如果你跟随损失到反向传播路径，可以使用它的 .grad_fn 属性，你将会看到一个这样的计算图：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">-&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">-&gt; MSELoss</span><br><span class="line">-&gt; loss</span><br></pre></td></tr></table></figure><p>所以，当我们调用 loss.backward ()，整个图都会微分，而且所有的在图中的 requires_grad=True 的张量将会让他们的 grad 张量累计梯度。</p><p>反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad() <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p>更新神经网络参数：</p><p>随机梯度下降：<br><code>weight = weight - learning_rate * gradient</code><br>python 实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>其它更新规则：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure><p>学到这暂时够用了，不够有后面再补充学习</p><div class="tags"><a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"><i class="ic i-tag"></i> 学习笔记</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 深度学习</a> <a href="/tags/PyTorch/" rel="tag"><i class="ic i-tag"></i> PyTorch</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-04-30 11:38:16" itemprop="dateModified" datetime="2025-04-30T11:38:16+08:00">2025-04-30</time> </span><span id="code/PyTorch学习笔记/" class="item leancloud_visitors" data-flag-title="PyTorch学习笔记" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Lemon Sour <i class="ic i-at"><em>@</em></i></li><li class="link"><strong>本文链接：</strong> <a href="http://amentiraz.github.io/code/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="PyTorch学习笔记">http://amentiraz.github.io/code/PyTorch学习笔记/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC96aC1DTg=="><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/article/STAGATE/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;amentirazblogpic.oss-cn-hangzhou.aliyuncs.com&#x2F;blogpic&#x2F;%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925162946.png" title="STAGATE"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 论文</span><h3>STAGATE</h3></a></div><div class="item right"><a href="/article/MENDER/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;amentirazblogpic.oss-cn-hangzhou.aliyuncs.com&#x2F;blogpic&#x2F;%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240925124332.png" title="MENDER"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 论文</span><h3>MENDER</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">1.</span> <span class="toc-text">PyTorch 自动微分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">PyTorch 神经网络</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/code/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="bookmark" title="PyTorch学习笔记">PyTorch学习笔记</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Lemon Sour" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Lemon Sour</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">93</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">26</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">57</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0FtZW50aXJheg==" title="https:&#x2F;&#x2F;github.com&#x2F;Amentiraz"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9kc2ZseS04" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;dsfly-8"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTE1MTc2ODUzMzM=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;1517685333"><i class="ic i-cloud-music"></i></span> <span class="exturl item email" data-url="bWFpbHRvOnZpb2xlbW9uQDE2My5jb20=" title="mailto:violemon@163.com"><i class="ic i-envelope"></i></span> <span class="exturl item bangumi" data-url="aHR0cHM6Ly9iYW5ndW1pLnR2L2FuaW1lL2xpc3QvNjY4MDE2" title="https:&#x2F;&#x2F;bangumi.tv&#x2F;anime&#x2F;list&#x2F;668016"><i class="ic i-bilibili"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-magic"></i>关于</a><ul class="submenu"><li class="item"><a href="/author/" rel="section"><i class="ic i-user"></i>本人</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-cloud"></i>其它</a><ul class="submenu"><li class="item"><a href="/music/" rel="section"><i class="ic i-music"></i>音乐区</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>朋友</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/article/STAGATE/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/article/MENDER/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/article/" title="分类于 论文">论文</a></div><span><a href="/article/domain%E5%86%85%E5%AE%B9%E7%9A%84%E6%80%BB%E7%BB%93/" title="对spatial domain内容的总结">对spatial domain内容的总结</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/life/" title="分类于 生活">生活</a> <i class="ic i-angle-right"></i> <a href="/categories/life/%E6%80%9D%E8%80%83/" title="分类于 思考">思考</a></div><span><a href="/life/%E6%88%96%E8%AE%B8%E6%88%91%E4%BB%AC%E6%B0%B8%E8%BF%9C%E6%97%A0%E6%B3%95%E8%8E%B7%E5%BE%97%E5%B9%B3%E9%9D%99/" title="或许我们永远无法获得平静">或许我们永远无法获得平静</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/article/" title="分类于 论文">论文</a></div><span><a href="/article/Benchmarking-spatial-claustering-methods-with-spatially-resolved-transcriptomic-data/" title="Benchmarking-spatial-claustering-methods-with-spatially-resolved-transcriptomic-data">Benchmarking-spatial-claustering-methods-with-spatially-resolved-transcriptomic-data</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/life/" title="分类于 生活">生活</a> <i class="ic i-angle-right"></i> <a href="/categories/life/%E6%91%98%E5%BD%95/" title="分类于 摘录">摘录</a></div><span><a href="/life/%E5%8F%A5%E5%AD%90%E6%91%98%E5%BD%95/" title="句子摘录">句子摘录</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 影视书籍">影视书籍</a> <i class="ic i-angle-right"></i> <a href="/categories/note/%E4%B9%A6/" title="分类于 书">书</a></div><span><a href="/note/%E5%B0%8F%E7%8E%8B%E5%AD%90%E4%B9%A6%E8%AF%84/" title="小王子书评">小王子书评</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/hello-world/" title="Hello World">Hello World</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/article/" title="分类于 论文">论文</a> <i class="ic i-angle-right"></i> <a href="/categories/article/%E6%95%B0%E6%8D%AE/" title="分类于 数据">数据</a></div><span><a href="/article/scanpy%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/" title="scanpy数据使用笔记">scanpy数据使用笔记</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/article/" title="分类于 论文">论文</a></div><span><a href="/article/DeepST/" title="DeepST">DeepST</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/other/" title="分类于 其它">其它</a></div><span><a href="/other/%E8%A7%82%E5%89%8D%E9%A1%BB%E7%9F%A5/" title="观前须知">观前须知</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/code/" title="分类于 代码">代码</a> <i class="ic i-angle-right"></i> <a href="/categories/code/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" title="分类于 数学建模">数学建模</a></div><span><a href="/code/%E6%8F%92%E5%80%BC%E7%AE%97%E6%B3%95/" title="插值算法">插值算法</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Lemon Sour @ Amentiraz</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"code/PyTorch学习笔记/",favicon:{show:"⁽⁽ଘ( ˊᵕˋ )ଓ⁾⁾",hide:"(つд⊂)"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->